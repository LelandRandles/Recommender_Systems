---
title: "DATA 612 - Summer 2020 - Project 5 | Implementing a Recommender System on Spark"
author: "Bruno de Melo and Leland Randles"
date: "July 9, 2020"
output: 
  html_document:
    toc: true # table of content true
    toc_float: true
    toc_depth: 3  # up to three depths of headings (specified by #, ## and ###)
    number_sections: true  #if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite
    highlight: tango  # specifies the syntax highlighting style
    #css: my.css   # you can add your custom css, should be in same folder
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(tidyverse)) install.packages("tidyverse",repos = "http://cran.us.r-project.org")
if(!require(recommenderlab)) install.packages("recommenderlab",repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("tidyverse",repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("recommenderlab",repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("recommenderlab",repos = "http://cran.us.r-project.org")
if(!require(sparklyr)) install.packages("DescTools",repos = "http://cran.us.r-project.org")
library("tidyverse")
library("recommenderlab")
library("knitr")
library("kableExtra")
library("ggplot2")
library("sparklyr")
```

# Implementing a Recommender System on Spark
<a href="#top"> Back To Top </a>  
  
In our previous assignment, we experimented with accuracy measures and incorporated serendipity into our recommender system. The dataset we used for that assignment - in its original form - had 10,000,000 ratings. We had scaled it down considerably for the purposes of that assignment. At this size, certain models would not even generate using `recommenderLab` due to local memory constraints.  
  
In this assignment, we will attempt to use the full dataset using Spark.  
  
```{r load_data}
# dataset of ratings
training_raw <- read.csv("C:/Users/Lelan/Documents/Education/CUNY/DATA612/Proj_4/training.csv") %>% data.frame()
# serendipidy dataset (which also contains ratings)
answers_raw <- read.csv("https://raw.githubusercontent.com/Randles-CUNY/DATA612/master/Proj_4/files_to_use/answers.csv") %>% data.frame()
colnames(answers_raw)[1] <- "userId"
# combinaton of ratings from training_raw and answers_raw
ratings_raw <- rbind(select(training_raw, 1:3), select(answers_raw, 1:3))
```  
  
## Move Dataframe to Spark 
  
Once the dataframe was created on the local disk, we connected to Spark and copied it to a Spark dataframe:  
  
```{r sprkload}
# install Spark
spark_install(version = "2.4", hadoop_version = "2.7")
# create connection to Spark
sc <- spark_connect(master = "local")  
# move local disk dataframe to Spark; had to split into 10 commands
ratingsSprk01 <- sdf_copy_to(sc, ratings_raw[1:10000,], "ratings_sprk01", overwrite = TRUE)

# ratingsSprk01 <- sdf_copy_to(sc, ratings_raw[1:1000000,], "ratings_sprk01", overwrite = TRUE)
# ratingsSprk02 <- sdf_copy_to(sc, ratings_raw[1000001:2000000,], "ratings_sprk02", overwrite = TRUE)
# ratingsSprk03 <- sdf_copy_to(sc, ratings_raw[2000001:3000000,], "ratings_sprk03", overwrite = TRUE)
# ratingsSprk04 <- sdf_copy_to(sc, ratings_raw[3000001:4000000,], "ratings_sprk04", overwrite = TRUE)
# ratingsSprk05 <- sdf_copy_to(sc, ratings_raw[4000001:5000000,], "ratings_sprk05", overwrite = TRUE)
# ratingsSprk06 <- sdf_copy_to(sc, ratings_raw[5000001:6000000,], "ratings_sprk06", overwrite = TRUE)
# ratingsSprk07 <- sdf_copy_to(sc, ratings_raw[6000001:7000000,], "ratings_sprk07", overwrite = TRUE)
# ratingsSprk08 <- sdf_copy_to(sc, ratings_raw[7000001:8000000,], "ratings_sprk08", overwrite = TRUE)
# ratingsSprk09 <- sdf_copy_to(sc, ratings_raw[8000001:9000000,], "ratings_sprk09", overwrite = TRUE)
# ratingsSprk10 <- sdf_copy_to(sc, ratings_raw[9000001:10000000,], "ratings_sprk10", overwrite = TRUE)
# combining into one Spark data frame
# ratingsSprk <- sdf_bind_rows(ratingsSprk01, ratingsSprk02, ratingsSprk03, ratingsSprk04, ratingsSprk05, ratingsSprk06, ratingsSprk07, ratingsSprk08, ratingsSprk09, ratingsSprk10, id = NULL)
```  
  
## Create Recommender Model in Spark  
  
```{r sprkmdl}
# create Spark recommender model
spark_mdl <- ml_als(ratingsSprk01, rating ~ userId + movieId, rating_col = "rating", user_col = "userId", item_col = "movieId", 10)
summary(spark_mdl)
spark_mdl$item_factors
spark_mdl$user_factors
# predictions
sparkPredict <- ml_predict(spark_mdl , spark_dataframe(ratingsSprk01))
# check accuracy

# disconnect
spark_disconnect(sc)
```
  
## Compare to Results from Project 4 Dataset
  

  
```{r rm}
training_subset <- read_csv("https://raw.githubusercontent.com/bsvmelo/Data612-Summer-2020/master/Project_4/training_subset.csv") %>% data.frame()
# subset as before
training_cols <- training_raw[,1:3]
ratings_dist <- distinct(training_cols)
ratings_rrm <- as(ratings_dist, "realRatingMatrix")
ratings <- ratings_rrm[,colCounts(ratings_rrm) > 200]
t_redux <- as(ratings, "data.frame")
colnames(t_redux) <- colnames(training_cols)
# create evaluation scheme
eval_sets <- evaluationScheme(data = ratings1, method = "cross-validation", k = 4, given = 5, goodRating = 3)
# build UBCF model and SVD model
ubcf_rec <- Recommender(getData(eval_sets, "train"), "UBCF", param = list(normalize = "center", method = "cosine"))
svd_rec <- Recommender(getData(eval_sets, "train"), "SVD", param = list(normalize = "center", k = 10))
# Make predictions with each model
ubcf_pred <- predict(ubcf_rec, getData(eval_sets, "known"), type = "ratings")
svd_pred <- predict(svd_rec, getData(eval_sets, "known"), type = "ratings")
```  
  
<br>
  
## Compare the UBCF and SVD Recommender Models to Spark Model
<a href="#top"> Back To Top </a>  
  
Now that we have built the two models, we will compare the errors and other metrics for each model:  

```{r err1}
# Table showing error calcs for UBCF vs SVD
ubcf_er <- calcPredictionAccuracy(ubcf_pred, getData(eval_sets, "unknown"))
svd_er <- calcPredictionAccuracy(svd_pred, getData(eval_sets, "unknown"))

# perhaps use R package Metrics to compute for sparkDF or perhaps ml_evaluator

# RMSE, MSE and MAE
k_Method <- c("UBCF-Cosine","SVD")
k_table_p <- data.frame(rbind(ubcf_er, svd_er)) # add spark metrics to table
rownames(k_table_p) <- k_Method
k_table_p <- k_table_p[order(k_table_p$RMSE ),]
kable(k_table_p) %>% kable_styling()
```   
  
<br>

# References
<a href="#top"> Back To Top </a>

* 