---
title: "DATA 612 - Summer 2020 - Project 4 | Accuracy and Beyond"
author: "Bruno de Melo and Leland Randles"
date: "July 2, 2020"
output: 
  html_document:
    toc: true # table of content true
    toc_float: true
    toc_depth: 3  # up to three depths of headings (specified by #, ## and ###)
    number_sections: true  #if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite
    highlight: tango  # specifies the syntax highlighting style
    #css: my.css   # you can add your custom css, should be in same folder
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(tidyverse)) install.packages("tidyverse",repos = "http://cran.us.r-project.org")
if(!require(recommenderlab)) install.packages("recommenderlab",repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("tidyverse",repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("recommenderlab",repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("recommenderlab",repos = "http://cran.us.r-project.org")
library("tidyverse")
library("recommenderlab")
library("knitr")
library("kableExtra")
library("ggplot2")
```

# Accuracy Comparisons (Deliverable 1)
<a href="#top"> Back To Top </a>  
  
In our previous assignment, we studied Matrix Factorization Methods. In this assignment, we will practice accuracy comparison methods and implement a user experience goal such as increased serendipity, novelty or diversity.  
  
For this assignment, we chose to use the Serendipity datasets here: https://grouplens.org/datasets/serendipity-2018/  
  
These datasets include 10,000,000 movie ratings. However, to make the dataset size more manageable, the ratings were reduced to include only ratings from the users who were part of the serendipity study (the "answers.csv" file). The `ratings_raw` dataset below ultimately contains 1,448,259 ratings.  
  
```{r load_data}
# load abbreviated datasets
# dataset of movies
movies_raw <- read.csv("https://raw.githubusercontent.com/Randles-CUNY/DATA612/master/Proj_4/files_to_use/movies_subset.csv")
# dataset containings recommendations from the recommender system created by authors
recommendations_raw <- read.csv("https://raw.githubusercontent.com/Randles-CUNY/DATA612/master/Proj_4/files_to_use/recommendations.csv")
# dataset of ratings
training_raw <- read.csv("C:/Users/Lelan/Documents/Education/CUNY/DATA612/Proj_4/files_to_use/training_subset.csv")
# serendipidy dataset (which also contains ratings)
answers_raw <- read.csv("https://raw.githubusercontent.com/Randles-CUNY/DATA612/master/Proj_4/files_to_use/answers.csv")
colnames(answers_raw)[1] <- "userId"
# combinaton of ratings from training_raw and answers_raw
ratings_raw <- rbind(select(training_raw, 1:3), select(answers_raw, 1:3))
ratings <- as(ratings_raw, "realRatingMatrix")
```  
  
## Create UBCF and SVD Recommender Models  
  
Per the assignment, Deliverable 1 asks us to build two recommender systems with our data. We chose to build a UBCF model and an SVD model.  
  
```{r rm}
# create evaluation scheme
set.seed(137)
eval_sets <- evaluationScheme(data = ratings, method = "cross-validation", k = 10, given = 15, goodRating = 4)
# build UBCF model and SVD model
ubcf_rec <- Recommender(getData(eval_sets, "train"), "UBCF", param = list(normalize = "center", method = "cosine"))
svd_rec <- Recommender(getData(eval_sets, "train"), "SVD", param = list(normalize = "center"))
# Make predictions with each model
ubcf_pred <- predict(ubcf_rec, getData(eval_sets, "known"), type = "ratings")
svd_pred <- predict(svd_rec, getData(eval_sets, "known"), type = "ratings")
```  
  
<br>
  
## Compare the UBCF and SVD Recommender Models 
<a href="#top"> Back To Top </a>  
  
Now that we have built the two models, we will compare the errors and other metrics for each model:  

```{r err1}
# Table showing error calcs for UBCF vs SVD
ubcf_er <- calcPredictionAccuracy(ubcf_pred, getData(eval_sets, "unknown"))
svd_er <- calcPredictionAccuracy(svd_pred, getData(eval_sets, "unknown"))
models_to_evaluate <- list(
  UBCF_cos = list(name = "UBCF", param = list(normalize = "center", method = "cosine")), 
  SVD = list(name = "SVD", param = list(normalize = "center"))
)
n_recommendations <- c(1, 5, seq(10, 100, 10))
list_results <- evaluate(x = eval_sets, method = models_to_evaluate, n = n_recommendations)
avg_matrices <- lapply(list_results, avg)
error_tables <- rbind(cbind(Model = rep("UBCF",12), n = rownames(avg_matrices$UBCF_cos), avg_matrices$UBCF_cos), cbind(Model = rep("SVD",12), n = rownames(avg_matrices$UBCF_cos), avg_matrices$SVD))
error_tables[,3:10] <- round(as.numeric(error_tables[,3:10]), 6)
kable(error_tables) %>% kable_styling()
```  
  
  
```{r err2}
k_Method <- c("UBCF-Cosine","SVD")
k_table_p <- data.frame(rbind(ubcf_er, svd_er))
rownames(k_table_p) <- k_Method
k_table_p <- k_table_p[order(k_table_p$RMSE ),]
kable(k_table_p) %>% kable_styling()
``` 
  
# References
<a href="#top"> Back To Top </a>

* [Building a Recommendation System with R by Suresh K. Gorakala, Michele Usuelli](https://www.amazon.com/dp/B012O8S1YM/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)